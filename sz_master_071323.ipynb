{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from plotly import tools\n",
    "# import plotly.plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from IPython.display import HTML, Image\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#importing the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "from platform import python_version\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, LSTM\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from keras import callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.layers import LeakyReLU, ReLU\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.optimize import minimize, differential_evolution, LinearConstraint, NonlinearConstraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7\n",
      "3.0.2\n",
      "1.6.1\n",
      "1.1.1\n"
     ]
    }
   ],
   "source": [
    "#check versions\n",
    "print(python_version())\n",
    "print(optuna.__version__)\n",
    "print(xgb.__version__)\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gather data from the .csv files for HTC and Pyrolysis.  Then process the data into features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'Pyrolysis_data.csv'\n",
    "data = pd.read_csv(filepath)\n",
    "p_X = data.iloc[:,[0,1,2,3,5,6,7,8,9,10]]\n",
    "p_y = data.iloc[:,11:19]\n",
    "p_X_train, p_X_test, p_y_train, p_y_test = train_test_split(p_X, p_y, test_size=0.2, random_state=100)\n",
    "\n",
    "filepath = 'HTC_data.csv'\n",
    "data = pd.read_csv(filepath)\n",
    "h_X = data.iloc[:,1:11]\n",
    "h_y = data.iloc[:,11:19]\n",
    "\n",
    "# split into training and test data\n",
    "h_X_train, h_X_test, h_y_train, h_y_test = train_test_split(h_X, h_y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technology type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = True # select true for pyrolysis, false for HTC\n",
    "c = True # True if CSI optimal solution is being used for the uncertainty analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets the X and y datasets used to either the pyrolysis or HTC data, depending on which dataset was selected in the above version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if p:\n",
    "    X = p_X\n",
    "    y = p_y\n",
    "else:\n",
    "    X = h_X\n",
    "    y = h_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna trial code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for the optuna trials.  Because Optuna is non-deterministic, optimal trial solutions found using this code will not result in the same hyperparamters reported in Table S3 and Table S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model\n",
    "## define an objective function\n",
    "def xgb_objective(trial):\n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "    r2_scores = []  \n",
    "    \n",
    "    for train, test in kfold.split(X, y):  \n",
    "        X_train_shuf, y_train_shuf = X.iloc[train], y.iloc[train]\n",
    "        X_test_shuf, y_test_shuf = X.iloc[test], y.iloc[test]\n",
    "        \n",
    "        subsample = trial.suggest_float('subsample', 0.5, 1.0, step=0.1)\n",
    "        colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0, step=0.1)\n",
    "        max_depth = trial.suggest_int('max_depth', 4, 10)\n",
    "        learning_rate = trial.suggest_categorical('learning_rate', [0.001, 0.01, 0.1, 0.3, 0.5])\n",
    "        n_estimators = trial.suggest_int('n_estimators', 1000, 5000, step=1000)\n",
    "        \n",
    "        regressor_XGB=xgb.XGBRegressor(learning_rate = learning_rate,\n",
    "                               n_estimators  = n_estimators,\n",
    "                               max_depth   = max_depth,\n",
    "                               subsample = subsample,\n",
    "                               colsample_bytree = colsample_bytree,\n",
    "                               reg_alpha = 0.00006,\n",
    "                               scale_pos_weight=1,\n",
    "                               eval_metric='rmse',\n",
    "                               random_state = 101)\n",
    "        XGB_model = MultiOutputRegressor(regressor_XGB)\n",
    "        XGB_model.fit(X_train_shuf, y_train_shuf)\n",
    "\n",
    "        predictions_XGB_test = XGB_model.predict(X_test_shuf)\n",
    "        predictions_XGB_test = pd.DataFrame(predictions_XGB_test, columns = y_test_shuf.columns)\n",
    "        \n",
    "        score_test = r2_score(y_test_shuf, predictions_XGB_test)\n",
    "        r2_scores.append(score_test)\n",
    "        \n",
    "    return np.mean(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_opt(X_train, y_train, X_test, y_test):\n",
    "    ## study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(xgb_objective, n_trials=60)\n",
    "\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('R2 score: {}'.format(trial.value))\n",
    "    print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "\n",
    "\n",
    "    regressor_XGB=xgb.XGBRegressor(\n",
    "                            learning_rate = trial.params['learning_rate'],\n",
    "                            n_estimators  = trial.params['n_estimators'],\n",
    "                            max_depth   = trial.params['max_depth'],\n",
    "                            subsample = trial.params['subsample'],\n",
    "                            colsample_bytree = trial.params['colsample_bytree'],\n",
    "                            reg_alpha = 0.00006,\n",
    "                            scale_pos_weight=1,\n",
    "                            eval_metric=mean_squared_error)\n",
    "    XGB_opt_model = MultiOutputRegressor(regressor_XGB)\n",
    "    XGB_opt_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    predictions_XGB_test = XGB_opt_model.predict(X_test)\n",
    "    predictions_XGB_test = pd.DataFrame(predictions_XGB_test, columns = y_test.columns)\n",
    "    score_test = r2_score(predictions_XGB_test, y_test)\n",
    "\n",
    "    for i in range(8):\n",
    "        print(f'The test score of column {y_test.columns[i]} is {r2_score(predictions_XGB_test.iloc[:,i], y_test.iloc[:,i])}')\n",
    "\n",
    "    for i in range(8):\n",
    "        print(f'The test RMSE of column {y_test.columns[i]} is {mean_squared_error(predictions_XGB_test.iloc[:,i], y_test.iloc[:,i], squared = False)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF model\n",
    "## define an objective function\n",
    "def rf_objective(trial):\n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "    r2_scores = []  \n",
    "    \n",
    "    for train, test in kfold.split(X, y):  \n",
    "        X_train_shuf, y_train_shuf = X.iloc[train], y.iloc[train]\n",
    "        X_test_shuf, y_test_shuf = X.iloc[test], y.iloc[test]\n",
    "        \n",
    "        n_estimators = trial.suggest_int('n_estimators', 1, 500)\n",
    "        max_features = trial.suggest_int('max_features', 1, 10)\n",
    "        max_depth = trial.suggest_int('max_depth', 1, 100)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 11)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 11)\n",
    "        regressor_RF = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                       max_features=max_features,\n",
    "                                       max_depth=max_depth,\n",
    "                                       min_samples_split=min_samples_split,\n",
    "                                       min_samples_leaf=min_samples_leaf,\n",
    "                                        criterion=\"mse\",\n",
    "                                      random_state=101)\n",
    "        model = MultiOutputRegressor(regressor_RF)\n",
    "        model.fit(X_train_shuf, y_train_shuf)\n",
    "        predictions_RF_test = model.predict(X_test_shuf)\n",
    "        predictions_RF_test = pd.DataFrame(predictions_RF_test, columns = y_test_shuf.columns)\n",
    "        score_test = r2_score(y_test_shuf, predictions_RF_test)\n",
    "        r2_scores.append(score_test)\n",
    "        \n",
    "    return np.mean(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_opt(X_train, y_train, X_test, y_test):\n",
    "    # Using optuna to optimize the hyperparameters\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(rf_objective, n_trials=60)\n",
    "\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('R2 score: {}'.format(trial.value))\n",
    "    print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "\n",
    "    regressor_RF=RandomForestRegressor(n_estimators= trial.params['n_estimators'], \n",
    "                                    max_features= trial.params['max_features'], \n",
    "                                    max_depth= trial.params['max_depth'], \n",
    "                                    min_samples_split= trial.params['min_samples_split'], \n",
    "                                    min_samples_leaf= trial.params['min_samples_leaf'], \n",
    "                                    random_state=101) # 7867\n",
    "    model = MultiOutputRegressor(regressor_RF)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions_RF_test = model.predict(X_test)\n",
    "    predictions_RF_test = pd.DataFrame(predictions_RF_test, columns = y_test.columns)\n",
    "    score_test = r2_score(predictions_RF_test, y_test)\n",
    "\n",
    "    for i in range(8):\n",
    "        print(f'The test score of column {y_test.columns[i]} is {r2_score(predictions_RF_test.iloc[:,i], y_test.iloc[:,i])}')\n",
    "\n",
    "    for i in range(8):\n",
    "        print(f'The test RMSE of column {y_test.columns[i]} is {mean_squared_error(predictions_RF_test.iloc[:,i], y_test.iloc[:,i], squared = False)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.001,  # minimium amount of change to count as an improvement\n",
    "    patience=20,  # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# R^2 Metric function\n",
    "def r_squared(y_true, y_pred):\n",
    "    SS_res = K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "\n",
    "def dnn_objective(trial):\n",
    "   \n",
    "    # Prepare cross validation\n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "   \n",
    "    r2_scores = []  # List to store R^2 score for each fold\n",
    "   \n",
    "    for train, test in kfold.split(X, y):  # assuming your data is in X and y\n",
    "        X_train_shuf, y_train_shuf = X.iloc[train], y.iloc[train]\n",
    "        X_test_shuf, y_test_shuf = X.iloc[test], y.iloc[test]\n",
    "        # Define Keras model\n",
    "        model = Sequential()\n",
    "        n_layers = trial.suggest_int('n_layers', 1, 5)\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            num_hidden = trial.suggest_int('n_units_layer{}'.format(i), 1, 768, log=True)\n",
    "            activation = trial.suggest_categorical('activation_{}'.format(i), ['relu', 'sigmoid', 'tanh', 'LeakyReLU'])\n",
    "\n",
    "            if i == 0:\n",
    "                model.add(Dense(num_hidden, kernel_initializer='uniform', activation=activation, input_dim=10))  # assuming input_dim=10\n",
    "            else:\n",
    "                model.add(Dense(num_hidden, activation=activation))\n",
    "\n",
    "        model.add(Dense(8, activation='linear'))  # output layer\n",
    "\n",
    "        # Compile model\n",
    "        lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "        opt = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=opt, loss='mse', metrics=[r_squared, 'mse'])  # r_squared is the custom R^2 function defined before\n",
    "       \n",
    "        # Fit model\n",
    "        history = model.fit(X_train_shuf, y_train_shuf, batch_size = 32, epochs = 500, \n",
    "                        validation_data=(X_test_shuf, y_test_shuf), verbose=0, callbacks=[early_stopping])  # assuming early_stopping is already defined\n",
    "\n",
    "        # Evaluate model\n",
    "        \n",
    "        predictions_DNN_test = model.predict(X_test_shuf)\n",
    "        predictions_DNN_test = pd.DataFrame(predictions_DNN_test, columns = y_test_shuf.columns)\n",
    "        score_test = r2_score(y_test_shuf, predictions_DNN_test)\n",
    "        r2_scores.append(score_test)\n",
    "       \n",
    "    return np.mean(r2_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_opt(X_train, y_train, X_test, y_test):\n",
    "    ## Start hyperparameters tuning using optuna\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(dnn_objective, n_trials=60)\n",
    "\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('R2 score: {}'.format(trial.value))\n",
    "    print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "\n",
    "    ## model after parameters tuning\n",
    "    DNN_opt_model = Sequential()\n",
    "    DNN_opt_model.add(Dense(units = trial.params['n_units_layer0'], activation = trial.params['activation_0'], input_dim = 10))\n",
    "\n",
    "    if trial.params['n_layers'] > 1:\n",
    "        DNN_opt_model.add(Dense(units = trial.params['n_units_layer1'], activation = trial.params['activation_1']))\n",
    "    if trial.params['n_layers'] > 2:\n",
    "        DNN_opt_model.add(Dense(units = trial.params['n_units_layer2'], activation = trial.params['activation_2']))\n",
    "    if trial.params['n_layers'] > 3:\n",
    "        DNN_opt_model.add(Dense(units = trial.params['n_units_layer3'], activation = trial.params['activation_3']))\n",
    "    if trial.params['n_layers'] > 4:\n",
    "        DNN_opt_model.add(Dense(units = trial.params['n_units_layer4'], activation = trial.params['activation_4']))\n",
    "\n",
    "\n",
    "    DNN_opt_model.add(Dense(units = 8, activation = 'linear'))\n",
    "    opt = Adam(learning_rate=trial.params['lr'])\n",
    "    DNN_opt_model.compile(optimizer = opt, loss = 'mse', metrics=[r_squared, 'mse'])\n",
    "    history = DNN_opt_model.fit(X_train, y_train, batch_size = 32, epochs = 500,validation_data=(X_test, y_test), verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # using model with optimized parameters to predict\n",
    "    predictions_DNN_test = DNN_opt_model.predict(X_test)\n",
    "    predictions_DNN_test = pd.DataFrame(predictions_DNN_test, columns = y_test.columns)\n",
    "    score_test = r2_score(predictions_DNN_test, y_test)\n",
    "\n",
    "    for i in range(8):\n",
    "        print(f'The test score of column {y_test.columns[i]} is {r2_score(predictions_DNN_test.iloc[:,i], y_test.iloc[:,i])}')\n",
    "\n",
    "    for i in range(8):\n",
    "        print(f'The test RMSE of column {y_test.columns[i]} is {mean_squared_error(predictions_DNN_test.iloc[:,i], y_test.iloc[:,i], squared = False)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Optuna Trials - S3-S4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-07-19 08:50:38,114]\u001b[0m A new study created in memory with name: no-name-0ec2142a-6045-4aff-8473-4fd700c2344e\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:51:30,558]\u001b[0m Trial 0 finished with value: 0.765958718398081 and parameters: {'n_estimators': 489, 'max_features': 10, 'max_depth': 88, 'min_samples_split': 11, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.765958718398081.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:52:04,153]\u001b[0m Trial 1 finished with value: 0.8245275101579349 and parameters: {'n_estimators': 318, 'max_features': 6, 'max_depth': 40, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.8245275101579349.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:52:09,595]\u001b[0m Trial 2 finished with value: 0.7958693969464181 and parameters: {'n_estimators': 60, 'max_features': 4, 'max_depth': 45, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.8245275101579349.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:52:28,927]\u001b[0m Trial 3 finished with value: 0.7264360837316384 and parameters: {'n_estimators': 256, 'max_features': 2, 'max_depth': 57, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.8245275101579349.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:52:30,230]\u001b[0m Trial 4 finished with value: 0.4014669565402986 and parameters: {'n_estimators': 15, 'max_features': 6, 'max_depth': 1, 'min_samples_split': 6, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.8245275101579349.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:53:10,714]\u001b[0m Trial 5 finished with value: 0.7509696738445677 and parameters: {'n_estimators': 421, 'max_features': 10, 'max_depth': 78, 'min_samples_split': 2, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.8245275101579349.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:53:41,544]\u001b[0m Trial 6 finished with value: 0.6705128163123291 and parameters: {'n_estimators': 398, 'max_features': 2, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.8245275101579349.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:54:16,323]\u001b[0m Trial 7 finished with value: 0.7563452777608048 and parameters: {'n_estimators': 386, 'max_features': 7, 'max_depth': 34, 'min_samples_split': 11, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.8245275101579349.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:54:52,869]\u001b[0m Trial 8 finished with value: 0.6783778419471578 and parameters: {'n_estimators': 470, 'max_features': 2, 'max_depth': 62, 'min_samples_split': 2, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.8245275101579349.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:55:31,019]\u001b[0m Trial 9 finished with value: 0.7290857798925604 and parameters: {'n_estimators': 422, 'max_features': 7, 'max_depth': 53, 'min_samples_split': 10, 'min_samples_leaf': 11}. Best is trial 1 with value: 0.8245275101579349.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:55:56,301]\u001b[0m Trial 10 finished with value: 0.8324506502460947 and parameters: {'n_estimators': 266, 'max_features': 4, 'max_depth': 26, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.8324506502460947.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:56:21,768]\u001b[0m Trial 11 finished with value: 0.8420695688312974 and parameters: {'n_estimators': 265, 'max_features': 4, 'max_depth': 24, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 11 with value: 0.8420695688312974.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:56:34,210]\u001b[0m Trial 12 finished with value: 0.8244973925729221 and parameters: {'n_estimators': 131, 'max_features': 4, 'max_depth': 22, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 11 with value: 0.8420695688312974.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:56:52,989]\u001b[0m Trial 13 finished with value: 0.84463307593709 and parameters: {'n_estimators': 197, 'max_features': 4, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:57:08,636]\u001b[0m Trial 14 finished with value: 0.8102132250977073 and parameters: {'n_estimators': 170, 'max_features': 4, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:57:23,411]\u001b[0m Trial 15 finished with value: 0.6237233076901039 and parameters: {'n_estimators': 195, 'max_features': 1, 'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 6}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:57:53,653]\u001b[0m Trial 16 finished with value: 0.831954593947134 and parameters: {'n_estimators': 315, 'max_features': 5, 'max_depth': 31, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:58:04,163]\u001b[0m Trial 17 finished with value: 0.7984268955761799 and parameters: {'n_estimators': 119, 'max_features': 3, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:58:25,902]\u001b[0m Trial 18 finished with value: 0.7791464153833526 and parameters: {'n_estimators': 193, 'max_features': 8, 'max_depth': 67, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:58:56,694]\u001b[0m Trial 19 finished with value: 0.8335845868946581 and parameters: {'n_estimators': 322, 'max_features': 5, 'max_depth': 37, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:59:16,973]\u001b[0m Trial 20 finished with value: 0.8096590111567558 and parameters: {'n_estimators': 230, 'max_features': 3, 'max_depth': 100, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 08:59:47,455]\u001b[0m Trial 21 finished with value: 0.8273269657977671 and parameters: {'n_estimators': 314, 'max_features': 5, 'max_depth': 39, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:00:22,208]\u001b[0m Trial 22 finished with value: 0.8430408511055358 and parameters: {'n_estimators': 352, 'max_features': 5, 'max_depth': 23, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:00:46,884]\u001b[0m Trial 23 finished with value: 0.8430575695733193 and parameters: {'n_estimators': 273, 'max_features': 3, 'max_depth': 22, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:01:15,835]\u001b[0m Trial 24 finished with value: 0.7843057431442026 and parameters: {'n_estimators': 356, 'max_features': 3, 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:01:32,462]\u001b[0m Trial 25 finished with value: 0.6414034076728955 and parameters: {'n_estimators': 227, 'max_features': 1, 'max_depth': 7, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:01:48,201]\u001b[0m Trial 26 finished with value: 0.8329198410831088 and parameters: {'n_estimators': 146, 'max_features': 7, 'max_depth': 47, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:01:56,664]\u001b[0m Trial 27 finished with value: 0.7958605451887697 and parameters: {'n_estimators': 94, 'max_features': 3, 'max_depth': 28, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:02:12,146]\u001b[0m Trial 28 finished with value: 0.4122604820855728 and parameters: {'n_estimators': 210, 'max_features': 6, 'max_depth': 1, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:02:40,984]\u001b[0m Trial 29 finished with value: 0.7669422850498341 and parameters: {'n_estimators': 286, 'max_features': 9, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:03:14,052]\u001b[0m Trial 30 finished with value: 0.7736798997228933 and parameters: {'n_estimators': 369, 'max_features': 5, 'max_depth': 21, 'min_samples_split': 7, 'min_samples_leaf': 6}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:03:41,038]\u001b[0m Trial 31 finished with value: 0.8333092687179781 and parameters: {'n_estimators': 283, 'max_features': 4, 'max_depth': 25, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:04:12,513]\u001b[0m Trial 32 finished with value: 0.8194359288617269 and parameters: {'n_estimators': 348, 'max_features': 3, 'max_depth': 31, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:04:34,657]\u001b[0m Trial 33 finished with value: 0.8309245270093524 and parameters: {'n_estimators': 236, 'max_features': 4, 'max_depth': 39, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:05:00,281]\u001b[0m Trial 34 finished with value: 0.8118631637894025 and parameters: {'n_estimators': 280, 'max_features': 5, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:05:13,631]\u001b[0m Trial 35 finished with value: 0.7896282656405613 and parameters: {'n_estimators': 161, 'max_features': 2, 'max_depth': 45, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:05:58,904]\u001b[0m Trial 36 finished with value: 0.8215215157458617 and parameters: {'n_estimators': 500, 'max_features': 6, 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:06:28,128]\u001b[0m Trial 37 finished with value: 0.8074751682361464 and parameters: {'n_estimators': 336, 'max_features': 4, 'max_depth': 44, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:06:51,787]\u001b[0m Trial 38 finished with value: 0.795343371954774 and parameters: {'n_estimators': 254, 'max_features': 6, 'max_depth': 34, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:07:29,485]\u001b[0m Trial 39 finished with value: 0.7828080020894379 and parameters: {'n_estimators': 456, 'max_features': 2, 'max_depth': 77, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.84463307593709.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:07:32,684]\u001b[0m Trial 40 finished with value: 0.8589876662568056 and parameters: {'n_estimators': 30, 'max_features': 3, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:07:35,427]\u001b[0m Trial 41 finished with value: 0.8458057603303845 and parameters: {'n_estimators': 25, 'max_features': 3, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:07:36,232]\u001b[0m Trial 42 finished with value: 0.8085539389452958 and parameters: {'n_estimators': 5, 'max_features': 3, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:07:39,914]\u001b[0m Trial 43 finished with value: 0.6486425407309702 and parameters: {'n_estimators': 42, 'max_features': 2, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 11}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:07:46,022]\u001b[0m Trial 44 finished with value: 0.8127890231344035 and parameters: {'n_estimators': 67, 'max_features': 3, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:07:50,862]\u001b[0m Trial 45 finished with value: 0.8484885822519548 and parameters: {'n_estimators': 48, 'max_features': 2, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:07:53,562]\u001b[0m Trial 46 finished with value: 0.7449004935624213 and parameters: {'n_estimators': 30, 'max_features': 1, 'max_depth': 35, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:07:59,498]\u001b[0m Trial 47 finished with value: 0.6627001168496427 and parameters: {'n_estimators': 77, 'max_features': 2, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 9}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:08:08,439]\u001b[0m Trial 48 finished with value: 0.8169169865286797 and parameters: {'n_estimators': 101, 'max_features': 1, 'max_depth': 53, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:08:13,233]\u001b[0m Trial 49 finished with value: 0.7827937373506624 and parameters: {'n_estimators': 48, 'max_features': 2, 'max_depth': 29, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:08:15,576]\u001b[0m Trial 50 finished with value: 0.7348673093577153 and parameters: {'n_estimators': 24, 'max_features': 3, 'max_depth': 12, 'min_samples_split': 4, 'min_samples_leaf': 7}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:08:16,228]\u001b[0m Trial 51 finished with value: 0.7857021129103677 and parameters: {'n_estimators': 3, 'max_features': 4, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:08:55,660]\u001b[0m Trial 52 finished with value: 0.8516449687134214 and parameters: {'n_estimators': 406, 'max_features': 4, 'max_depth': 27, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:09:33,976]\u001b[0m Trial 53 finished with value: 0.8395113531817933 and parameters: {'n_estimators': 442, 'max_features': 2, 'max_depth': 33, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:10:10,926]\u001b[0m Trial 54 finished with value: 0.8235885532339605 and parameters: {'n_estimators': 403, 'max_features': 4, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:10:18,149]\u001b[0m Trial 55 finished with value: 0.708260582449254 and parameters: {'n_estimators': 91, 'max_features': 3, 'max_depth': 41, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:10:23,086]\u001b[0m Trial 56 finished with value: 0.847587528590091 and parameters: {'n_estimators': 51, 'max_features': 4, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:10:28,641]\u001b[0m Trial 57 finished with value: 0.8364468293631729 and parameters: {'n_estimators': 58, 'max_features': 4, 'max_depth': 18, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:10:41,286]\u001b[0m Trial 58 finished with value: 0.8580282520424662 and parameters: {'n_estimators': 119, 'max_features': 5, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n",
      "\u001b[32m[I 2023-07-19 09:10:51,225]\u001b[0m Trial 59 finished with value: 0.8013567156626277 and parameters: {'n_estimators': 113, 'max_features': 5, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 40 with value: 0.8589876662568056.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.8589876662568056\n",
      "Best hyperparameters: {'n_estimators': 30, 'max_features': 3, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
      "The test score of column Yield_char (%) is 0.8848429086054845\n",
      "The test score of column C_char (%) is 0.9451500301951765\n",
      "The test score of column H_char (%) is 0.8245715688458277\n",
      "The test score of column O_char (%) is 0.8724289900513464\n",
      "The test score of column N_char (%) is 0.9363171598592425\n",
      "The test score of column Char_HHV is 0.8244862111259328\n",
      "The test score of column ER is 0.7531198895691602\n",
      "The test score of column CR is 0.8044920483444091\n",
      "The test RMSE of column Yield_char (%) is 4.233079664167528)\n",
      "The test RMSE of column C_char (%) is 3.15465585935914)\n",
      "The test RMSE of column H_char (%) is 0.4837383577518897)\n",
      "The test RMSE of column O_char (%) is 3.2093223214687705)\n",
      "The test RMSE of column N_char (%) is 0.22622916047985228)\n",
      "The test RMSE of column Char_HHV is 2.2705150264483147)\n",
      "The test RMSE of column ER is 6.078209026047578)\n",
      "The test RMSE of column CR is 5.83788303797661)\n"
     ]
    }
   ],
   "source": [
    "'''Uncomment the trials that you wish to run'''\n",
    "\n",
    "#HTC\n",
    "#XGB_opt(h_X_train, h_y_train, h_X_test, h_y_test)\n",
    "#RF_opt(h_X_train, h_y_train, h_X_test, h_y_test)\n",
    "#DNN_opt(h_X_train, h_y_train, h_X_test, h_y_test)\n",
    "\n",
    "# pyrolysis\n",
    "#XGB_opt(p_X_train, p_y_train, p_X_test, p_y_test)\n",
    "RF_opt(p_X_train, p_y_train, p_X_test, p_y_test)\n",
    "#DNN_opt(p_X_train, p_y_train, p_X_test, p_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal models code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_XGB(X_train, y_train, X_test, y_test, params):\n",
    "    regressor_XGB=xgb.XGBRegressor(\n",
    "                            learning_rate = params[3],\n",
    "                            n_estimators  = params[4],\n",
    "                            max_depth   = params[0],\n",
    "                            subsample = params[2],\n",
    "                            colsample_bytree = params[1],\n",
    "                            reg_alpha = 0.00006,\n",
    "                            scale_pos_weight=1,\n",
    "                            eval_metric=mean_squared_error)\n",
    "    XGB_opt_model = MultiOutputRegressor(regressor_XGB)\n",
    "    XGB_opt_model.fit(X_train, y_train)\n",
    "\n",
    "    predictions_XGB_test = XGB_opt_model.predict(X_test)\n",
    "    predictions_XGB_test = pd.DataFrame(predictions_XGB_test, columns = y_test.columns)\n",
    "    score_test = r2_score(predictions_XGB_test, y_test)\n",
    "\n",
    "    print(\"XGB\")\n",
    "\n",
    "    for i in range(8):\n",
    "        print('The test score of column {} is {}'.format(y_test.columns[i], r2_score(predictions_XGB_test.iloc[:,i], y_test.iloc[:,i])))\n",
    "\n",
    "    for i in range(8):\n",
    "        print('The test RMSE of column {} is {}'.format(y_test.columns[i], mean_squared_error(predictions_XGB_test.iloc[:,i], y_test.iloc[:,i], squared = False)))\n",
    "\n",
    "    return XGB_opt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_RF(X_train, y_train, X_test, y_test, params):\n",
    "    regressor_RF=RandomForestRegressor(n_estimators= params[0], \n",
    "                                    max_features= params[1], \n",
    "                                    max_depth= params[2], \n",
    "                                    min_samples_split= params[3], \n",
    "                                    min_samples_leaf= params[4], \n",
    "                                    random_state=101) # 7867\n",
    "    RF_opt_model = MultiOutputRegressor(regressor_RF)\n",
    "    RF_opt_model.fit(X_train, y_train)\n",
    "\n",
    "    predictions_RF_test = RF_opt_model.predict(X_test)\n",
    "    predictions_RF_test = pd.DataFrame(predictions_RF_test, columns = y_test.columns)\n",
    "    score_test = r2_score(predictions_RF_test, y_test)\n",
    "\n",
    "    print(\"RF\")\n",
    "\n",
    "    for i in range(8):\n",
    "        print('The test score of column {} is {}'.format(y_test.columns[i], r2_score(predictions_RF_test.iloc[:,i], y_test.iloc[:,i])))\n",
    "\n",
    "    for i in range(8):\n",
    "        print('The test RMSE of column {} is {}'.format(y_test.columns[i], mean_squared_error(predictions_RF_test.iloc[:,i], y_test.iloc[:,i], squared = False)))\n",
    "\n",
    "    return RF_opt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_DNN(X_train, y_train, X_test, y_test, params):\n",
    "    DNN_opt_model = Sequential()\n",
    "\n",
    "    DNN_opt_model.add(Dense(units = params[1], kernel_initializer='uniform', activation = params[4], input_dim = 10))\n",
    "    DNN_opt_model.add(Dense(units = params[2], activation = params[5]))\n",
    "    DNN_opt_model.add(Dense(units = params[3], activation = params[6]))\n",
    "    DNN_opt_model.add(Dense(units = 8, activation = 'linear'))\n",
    "    opt = Adam(learning_rate=params[0])\n",
    "    DNN_opt_model.compile(optimizer = opt, loss = 'mse', metrics=[r_squared, 'mse'])\n",
    "    history =DNN_opt_model.fit(X_train, y_train, batch_size = 32, validation_data=(X_test, y_test), verbose = 0, epochs = 500, callbacks=[early_stopping])\n",
    "\n",
    "    predictions_DNN_test = DNN_opt_model.predict(X_test)\n",
    "    predictions_DNN_test = pd.DataFrame(predictions_DNN_test, columns = y_test.columns)\n",
    "    score_test = r2_score(predictions_DNN_test, y_test)\n",
    "\n",
    "    print(\"DNN\")\n",
    "\n",
    "    for i in range(8):\n",
    "        print('The test score of column {} is {}'.format(y_test.columns[i], r2_score(predictions_DNN_test.iloc[:,i], y_test.iloc[:,i])))\n",
    "\n",
    "    for i in range(8):\n",
    "        print('The test RMSE of column {} is {}'.format(y_test.columns[i], mean_squared_error(predictions_DNN_test.iloc[:,i], y_test.iloc[:,i], squared = False)))\n",
    "\n",
    "    return DNN_opt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Table S5-S6, Fig 3., Fig. 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyrolysis\n",
      "XGB\n",
      "The test score of column Yield_char (%) is 0.9373907052076876\n",
      "The test score of column C_char (%) is 0.9806448346312376\n",
      "The test score of column H_char (%) is 0.9184176543296934\n",
      "The test score of column O_char (%) is 0.8951628085735539\n",
      "The test score of column N_char (%) is 0.9335559156881511\n",
      "The test score of column Char_HHV is 0.8809580613340434\n",
      "The test score of column ER is 0.6877782356201139\n",
      "The test score of column CR is 0.8792999874255543\n",
      "The test RMSE of column Yield_char (%) is 3.5302740947490587\n",
      "The test RMSE of column C_char (%) is 2.0504959685201762\n",
      "The test RMSE of column H_char (%) is 0.37602793757767167\n",
      "The test RMSE of column O_char (%) is 3.140153966521116\n",
      "The test RMSE of column N_char (%) is 0.227617947447542\n",
      "The test RMSE of column Char_HHV is 1.9806617023981103\n",
      "The test RMSE of column ER is 7.8265221454589415\n",
      "The test RMSE of column CR is 5.173733178784443\n",
      "RF\n",
      "The test score of column Yield_char (%) is 0.9009704210779682\n",
      "The test score of column C_char (%) is 0.9629208093648147\n",
      "The test score of column H_char (%) is 0.8215593766283649\n",
      "The test score of column O_char (%) is 0.8564593907738485\n",
      "The test score of column N_char (%) is 0.9346993955002862\n",
      "The test score of column Char_HHV is 0.8314691950456509\n",
      "The test score of column ER is 0.7188000511196257\n",
      "The test score of column CR is 0.8268233953845816\n",
      "The test RMSE of column Yield_char (%) is 4.150978909572448\n",
      "The test RMSE of column C_char (%) is 2.693204151536904\n",
      "The test RMSE of column H_char (%) is 0.5082148179135635\n",
      "The test RMSE of column O_char (%) is 3.58354634194663\n",
      "The test RMSE of column N_char (%) is 0.22757187583391633\n",
      "The test RMSE of column Char_HHV is 2.2881594354533794\n",
      "The test RMSE of column ER is 6.786720580163861\n",
      "The test RMSE of column CR is 5.823197548859713\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "DNN\n",
      "The test score of column Yield_char (%) is 0.8690269173902159\n",
      "The test score of column C_char (%) is 0.9107864714142421\n",
      "The test score of column H_char (%) is 0.7445505420623569\n",
      "The test score of column O_char (%) is 0.7849280492942483\n",
      "The test score of column N_char (%) is 0.052354627408918164\n",
      "The test score of column Char_HHV is 0.6439307331193238\n",
      "The test score of column ER is 0.27224315082740613\n",
      "The test score of column CR is 0.7940461923515061\n",
      "The test RMSE of column Yield_char (%) is 4.869378840156736\n",
      "The test RMSE of column C_char (%) is 4.107728370684103\n",
      "The test RMSE of column H_char (%) is 0.6218718809390035\n",
      "The test RMSE of column O_char (%) is 4.208946221534771\n",
      "The test RMSE of column N_char (%) is 0.7425071332266426\n",
      "The test RMSE of column Char_HHV is 3.4061849960927972\n",
      "The test RMSE of column ER is 8.823655461767885\n",
      "The test RMSE of column CR is 6.446524002506914\n",
      "HTC\n",
      "XGB\n",
      "The test score of column Char_yield (%) is 0.9253504507241267\n",
      "The test score of column Char_C (%) is 0.9485470706635613\n",
      "The test score of column Char_H (%) is 0.9455605410428528\n",
      "The test score of column Char_N (%) is 0.9439318053282585\n",
      "The test score of column Char_O (%) is 0.9005807490064875\n",
      "The test score of column HHV (MJ/kg) is 0.929236054051294\n",
      "The test score of column ER (%) is 0.9080653543314965\n",
      "The test score of column CR (%) is 0.9175344447308332\n",
      "The test RMSE of column Char_yield (%) is 5.669157970311885\n",
      "The test RMSE of column Char_C (%) is 2.7037494816028373\n",
      "The test RMSE of column Char_H (%) is 0.2873197990334975\n",
      "The test RMSE of column Char_N (%) is 0.47034229837345426\n",
      "The test RMSE of column Char_O (%) is 2.919786755429174\n",
      "The test RMSE of column HHV (MJ/kg) is 1.3900838419787696\n",
      "The test RMSE of column ER (%) is 6.903565783783382\n",
      "The test RMSE of column CR (%) is 6.356600937168214\n",
      "RF\n",
      "The test score of column Char_yield (%) is 0.851411259292584\n",
      "The test score of column Char_C (%) is 0.9306406944657376\n",
      "The test score of column Char_H (%) is 0.9124042799858536\n",
      "The test score of column Char_N (%) is 0.9302868415977541\n",
      "The test score of column Char_O (%) is 0.8502406530812413\n",
      "The test score of column HHV (MJ/kg) is 0.8990248383849441\n",
      "The test score of column ER (%) is 0.8483899468962165\n",
      "The test score of column CR (%) is 0.8495863724865617\n",
      "The test RMSE of column Char_yield (%) is 7.272323754221924\n",
      "The test RMSE of column Char_C (%) is 2.965209789845745\n",
      "The test RMSE of column Char_H (%) is 0.3393190323996846\n",
      "The test RMSE of column Char_N (%) is 0.5128840633474478\n",
      "The test RMSE of column Char_O (%) is 3.237109253572986\n",
      "The test RMSE of column HHV (MJ/kg) is 1.5391002162923009\n",
      "The test RMSE of column ER (%) is 8.125985684793891\n",
      "The test RMSE of column CR (%) is 7.810769978328319\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "DNN\n",
      "The test score of column Char_yield (%) is 0.8352514393087875\n",
      "The test score of column Char_C (%) is 0.7774766979518871\n",
      "The test score of column Char_H (%) is 0.3918499984319229\n",
      "The test score of column Char_N (%) is 0.7529893367721977\n",
      "The test score of column Char_O (%) is 0.5153104964444903\n",
      "The test score of column HHV (MJ/kg) is 0.6617032448363742\n",
      "The test score of column ER (%) is 0.7843505767158707\n",
      "The test score of column CR (%) is 0.7896825099699396\n",
      "The test RMSE of column Char_yield (%) is 8.18479831958505\n",
      "The test RMSE of column Char_C (%) is 5.370057699813994\n",
      "The test RMSE of column Char_H (%) is 0.8095816552722326\n",
      "The test RMSE of column Char_N (%) is 0.9728903761464712\n",
      "The test RMSE of column Char_O (%) is 6.370617123078533\n",
      "The test RMSE of column HHV (MJ/kg) is 2.7942063796499674\n",
      "The test RMSE of column ER (%) is 10.009980714105545\n",
      "The test RMSE of column CR (%) is 9.641708325189663\n"
     ]
    }
   ],
   "source": [
    "#these are the optimal hyperparameter identified during an optuna trial and reported in Table S3 and S4.  \n",
    "# Please refer to the tables in the paper for the column heading for each of the parameters\n",
    "h_xgb_params = [3, .5, .8, .01, 5000]\n",
    "h_rf_params = [92, 2, 48, 2, 1]\n",
    "h_dnn_params = [.001, 672, 224, 480, 'LeakyReLU', 'ReLU', 'LeakyReLU']\n",
    "p_xgb_params = [4, .6, .6, .01, 3000]\n",
    "p_rf_params = [100, 6, 16, 2, 1]\n",
    "p_dnn_params = [.000358, 415, 679, 282, 'LeakyReLU', 'ReLU', 'LeakyReLU']\n",
    "\n",
    "print(\"Pyrolysis\")\n",
    "p_opt_XGB = optimal_XGB(p_X_train, p_y_train, p_X_test, p_y_test, p_xgb_params)\n",
    "p_opt_RF = optimal_RF(p_X_train, p_y_train, p_X_test, p_y_test, p_rf_params)\n",
    "p_opt_DNN = optimal_DNN(p_X_train, p_y_train, p_X_test, p_y_test, p_dnn_params)\n",
    "\n",
    "print(\"HTC\")\n",
    "h_opt_XGB = optimal_XGB(h_X_train, h_y_train, h_X_test, h_y_test, h_xgb_params)\n",
    "h_opt_RF = optimal_RF(h_X_train, h_y_train, h_X_test, h_y_test, h_rf_params)\n",
    "h_opt_DNN = optimal_DNN(h_X_train, h_y_train, h_X_test, h_y_test, h_dnn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto Front - Fig. 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(FW_C):\n",
    "    # Your model code here\n",
    "    if p:\n",
    "        model  = p_opt_XGB.predict(FW_C)\n",
    "    else:\n",
    "        model  = h_opt_XGB.predict(FW_C)\n",
    "    return model \n",
    "\n",
    "# Define the objectives and constraints\n",
    "def f1(x):\n",
    "    inputs = x.reshape(1, -1)\n",
    "    outputs = my_model(inputs)\n",
    "\n",
    "    #these equations differ due to the different orders of output for the labels\n",
    "    if p:\n",
    "        return outputs[:, 7] - (outputs[:, 2]/ outputs[:, 1]*12 + outputs[:, 3]/ outputs[:, 1]*12/16 + outputs[:, 4]/ outputs[:, 1]*12/14) / 3 * outputs[:, 0]\n",
    "    else:\n",
    "        return outputs[:, 7] - (outputs[:, 2]/ outputs[:, 1]*12 + outputs[:, 3]/ outputs[:, 1]*12/14 + outputs[:, 4]/ outputs[:, 1]*12/16) / 3 * outputs[:, 0]\n",
    "\n",
    "# ORDER OF VALUES FOR INPUTS AND OUTPUTS\n",
    "#                   0           1           2           3       4           5       6  7\n",
    "#y pyrolysis:  Yield_char (%),C_char (%),H_char (%),O_char (%),N_char (%),Char_HHV,ER,CR\n",
    "#              0          1         2           3         4          5           6      7\n",
    "#y htc: Char_yield (%),Char_C (%),Char_H (%),Char_N (%),Char_O (%),HHV (MJ/kg),ER (%),CR (%)\n",
    "#          0    1     2     3      4     5       6     7       8       9\n",
    "#x htc: C (%),H (%),N (%),O (%),WC (%),VM (%),FC (%),Ash (%),T (C),RT (min)\n",
    "#                 0        1      2       3     4     5       6       7      8          9\n",
    "#x pyrolysis: C (wt%),H (wt%),N (wt%),O (wt%),FC (%),VM (%),Ash (%),T (C),RT (min),HT (C/min)\n",
    "\n",
    "# REI = HHV/(0.75 * (Power * time) + mH2O*(H^T_H2O-25) + mbiomass * 5.34t^2-299t - 5.34*298^2-299*298\n",
    "\n",
    "def f2(x):\n",
    "    inputs = x.reshape(1, -1)\n",
    "    outputs = my_model(inputs)\n",
    "\n",
    "    if p:\n",
    "        #print(\"x array\", x)\n",
    "        power = 1 #kW\n",
    "        time = (inputs[:, 8] + (inputs[:, 7] - 25)/inputs[:, 9]) * 60 # seconds\n",
    "        Q_e = power * time # KJ\n",
    "\n",
    "\n",
    "        m_biomass = 1 #kg\n",
    "        m_h2o = 0.05 #kg\n",
    "        Q_p_biomass = m_biomass*(2.67*((273+inputs[:, 7])**2-298**2) - 299*(273+inputs[:, 7]-298))/1000 + m_h2o * 40.65 * 1000/18#, specific heat is 5.34 kJ/kg/degree C, but is divided by 2 during integration yields kJ , 1000/18 moles water per kg, yields 113 KJ\n",
    "        #print(\"q_p_biomass (KJ)\", Q_p_biomass)\n",
    "\n",
    "        Q_p_h2o = m_h2o * 4.186 * (inputs[:, 7] - 25)  #unitless * kg * kj/mol * mol/kg, 4.186 kJ/kg/degree C\n",
    "        #print(\"q_p_h2o (KJ)\", Q_p_h2o)\n",
    "\n",
    "        Q_p = Q_p_h2o + Q_p_biomass # KJ  + KJ = KJ\n",
    "        \n",
    "\n",
    "        Q_c = 1000*outputs[:, 5] # KJ/kg\n",
    "        #print(\"Q_c (KJ/kg)\", Q_c)\n",
    "\n",
    "        rei = Q_c/(0.75*Q_e+Q_p)\n",
    "        #print(\"REI (1/kg)\", rei)\n",
    "        #print(\"old objective\", outputs[:, 5] * 1000 / (45 * (inputs[:, 8] + (inputs[:, 7] - 25)/inputs[:, 9] ) + 113 + (inputs[:, 7] - 25) * (4.186 * 0.05 + 0.00267 * inputs[:, 7] + 1.22557) ))\n",
    "        #print(\"Q_e (KJ)\", Q_e)\n",
    "        #print(\"old Q_e\", 60*(inputs[:, 8] + (inputs[:, 7] - 25)/inputs[:, 9] ))\n",
    "        #print(\"Q_p (KJ)\", Q_p)\n",
    "        #print(\"old Q_p\", (113 + (inputs[:, 7] - 25) * (4.186 * 0.05 + 0.00267 * inputs[:, 7] + 1.22557)))\n",
    "        #print(\"new denominator\", (0.75*Q_e+Q_p))\n",
    "        #print(\"old denominator\", (45 * (inputs[:, 8] + (inputs[:, 7] - 25)/inputs[:, 9] ) + 113 + (inputs[:, 7] - 25) * (4.186 * 0.05 + 0.00267 * inputs[:, 7] + 1.22557) ))\n",
    "\n",
    "        return outputs[:, 5] * 1000 / (45 * (inputs[:, 8] + (inputs[:, 7] - 25)/inputs[:, 9] ) + 113 + (inputs[:, 7] - 25) * (4.186 * 0.05 + 0.00267 * inputs[:, 7] + 1.22557) )\n",
    "    else:\n",
    "        power = 1 #kW\n",
    "        time = inputs[:, 9] *60 #seconds\n",
    "        Q_e = power * time #KJ\n",
    "        #print(\"Q_e\", Q_e)\n",
    "\n",
    "        m_biomass = 1 #kg\n",
    "        Q_p_biomass = m_biomass*(2.67*((273+inputs[:, 8])**2-298**2) - 299*(273+inputs[:, 8]-298))/1000 #specific heat is 2.67 kJ/kg/degree C, yields kJ\n",
    "        #print(\"q_p_biomass\", Q_p_biomass)\n",
    "\n",
    "        m_h2o = inputs[:, 4]/(100-inputs[:, 4])*m_biomass #kg water/kg biomass * 100/(100- kg water/kg biomass) yields kg water ????\n",
    "        Q_p_h2o = m_h2o * 4.186 * (inputs[:, 8] - 25)  #unitless * kg * kj/mol * mol/kg, 4.186 kJ/kg/degree C\n",
    "        #print(\"q_p_h2o\", Q_p_h2o)\n",
    "\n",
    "        Q_p = Q_p_h2o + Q_p_biomass # KJ  + KJ = KJ\n",
    "        #print(\"Q_p\", Q_p)\n",
    "\n",
    "        Q_c = 1000*outputs[:, 5] # KJ/kg\n",
    "        #print(\"Q_c\", Q_c)\n",
    "\n",
    "        rei = Q_c/(0.75*Q_e+Q_p)\n",
    "        #print(\"REI\", rei)\n",
    "        #print(\"old objective\", (outputs[:, 5]+10) * 1000 / (45 * inputs[:, 9] + (inputs[:, 8] - 25) * (4.186 * inputs[:, 4] / (100 - inputs[:, 4]) + 0.00267 * inputs[:, 8] + 1.22557)))\n",
    "        #print(\"denominator\", (0.75*Q_e+Q_p))\n",
    "        #print(\"old denominator\", (45 * inputs[:, 9] + (inputs[:, 8] - 25) * (4.186 * inputs[:, 4] / (100 - inputs[:, 4]) + 0.00267 * inputs[:, 8] + 1.22557)))\n",
    "\n",
    "        return (outputs[:, 5]+10) * 1000 / (45 * inputs[:, 9] + (inputs[:, 8] - 25) * (4.186 * inputs[:, 4] / (100 - inputs[:, 4]) + 0.00267 * inputs[:, 8] + 1.22557))\n",
    "\n",
    "def c1(x):\n",
    "    return x[0] + x[1] + x[2] + x[3] + x[7] - 100 - 2\n",
    "\n",
    "def c2(x):\n",
    "    return -x[0] - x[1] - x[2] - x[3] - x[7] + 100 - 2\n",
    "\n",
    "def c3(x):\n",
    "    return x[5] + x[6] + x[7] - 100 - 2\n",
    "\n",
    "def c4(x):\n",
    "    return -x[5] - x[6] - x[7] + 100 - 2\n",
    "\n",
    "def c_f2(x):\n",
    "    # Compute the secondary objective\n",
    "    f2_value = f2(x)\n",
    "    # Return the constraint value\n",
    "    return f2_value - f2_max\n",
    "\n",
    "def c_f1(x):\n",
    "    # Compute the secondary objective\n",
    "    f1_value = f1(x)\n",
    "    # Return the constraint value\n",
    "    return f1_value - f1_max\n",
    "\n",
    "\n",
    "# Define the epsilon penalty function\n",
    "def epsilon_penalty_f2(x):\n",
    "    # Compute the secondary objective\n",
    "    f2_value = f2(x)\n",
    "    # Compute the epsilon penalty\n",
    "    if f2_value > f2_max:\n",
    "#         return epsilon * (f2_value - f2_max)\n",
    "        return 0\n",
    "    else:\n",
    "#         return 0\n",
    "        return epsilon * (f2_value)\n",
    "\n",
    "# Define the epsilon penalty function\n",
    "def epsilon_penalty_f1(x):\n",
    "    # Compute the secondary objective\n",
    "    f1_value = f1(x)\n",
    "    # Compute the epsilon penalty\n",
    "    if f1_value > f1_max:\n",
    "        return 0\n",
    "#         return epsilon * (f1_value - f1_max)\n",
    "    else:\n",
    "        return epsilon * (f1_value)\n",
    "\n",
    "    \n",
    "# Define the optimization problem\n",
    "def opt_f2(x):\n",
    "    f1_value = f1(x)\n",
    "    f2_value = f2(x)\n",
    "    return -(f1_value + epsilon_penalty_f2(x))\n",
    "\n",
    "def opt_f1(x):\n",
    "    f1_value = f1(x)\n",
    "    f2_value = f2(x)\n",
    "    return -(f2_value + epsilon_penalty_f1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-objective optimization - Table 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code yields the 6 points on the optimal pareto front.  To change technologies, change the boolean p in the Technology Type Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "(array([48.66001], dtype=float32), array([17.07721336]))\n",
      "(array([58.52834], dtype=float32), array([13.59999639]))\n",
      "(array([58.52834], dtype=float32), array([13.09715679]))\n",
      "(array([59.117287], dtype=float32), array([12.59185963]))\n",
      "(array([59.117287], dtype=float32), array([12.19844018]))\n",
      "(array([59.117287], dtype=float32), array([12.95433507]))\n",
      "[ 53.9          8.9          1.7         33.7         13.8\n",
      "  84.8          1.4        201.23045681   5.00154107  17.38490306]\n",
      "[ 53.9          8.9          1.7         33.7         13.8\n",
      "  84.8          1.4        316.3499131    5.20303568  14.40668601]\n",
      "[ 53.9          8.9          1.7         33.7         13.8\n",
      "  84.8          1.4        320.24192242   5.73425469  14.01514389]\n",
      "[ 53.9          8.9          1.7         33.7         13.8\n",
      "  84.8          1.4        316.70236027   6.46385857  12.47812905]\n",
      "[ 53.9          8.9          1.7         33.7         13.8\n",
      "  84.8          1.4        322.3539288    6.7905429   12.29524736]\n",
      "[ 53.9          8.9          1.7         33.7         13.8\n",
      "  84.8          1.4        315.68217847   5.22379077  12.43955088]\n"
     ]
    }
   ],
   "source": [
    "# # Define the bounds\n",
    "if p:\n",
    "    #                 0          1           2            3             4            5              6        7            8          9\n",
    "    #x pyrolysis: C (wt%),   H (wt%),     N (wt%),     O (wt%),       FC (%),     VM (%),       Ash (%),   T (C),    RT (min), HT (C/min)\n",
    "    bounds = [(53.9, 53.9), (8.9, 8.9), (1.7, 1.7), (33.7, 33.7), (13.8, 13.8), (84.8, 84.8), (1.4, 1.4), (200, 500), (5, 120), (2.5, 20)]\n",
    "\n",
    "    # Set the upper bound for the secondary objective\n",
    "    maxes = [50, 13.6, 13.1, 12.6, 12.2, 20] #the first entry in this is for HC1, with f1_max.  The other entriest are for HC2-6 with F2 max\n",
    "\n",
    "    # Define the epsilon values\n",
    "    epsilons = [1, 9, 6, 3, 2, .001] #the first entry in this is for HC1, with f1_max.  The other entriest are for HC2-6 with F2 max\n",
    "else:\n",
    "    #           0                1          2              3         4              5               6           7           8       9\n",
    "    #x htc:    C (%),          H (%),       N (%),     O (%),       WC (%),      VM (%),        FC (%),     Ash (%),     T (C),    RT (min)\n",
    "    bounds = [(53.9, 53.9), (8.9, 8.9), (1.7, 1.7), (33.7, 33.7), (50.4, 96), (84.8, 84.8), (13.8, 13.8), (1.4, 1.4), (180, 250), (30, 240)]\n",
    "\n",
    "    # Set the upper bound for the secondary objective\n",
    "    maxes =  [55, 12, 10, 9, 8, 20]  #the first entry in this is for HC1, with f1_max.  The other entriest are for HC2-6 with F2 max\n",
    "\n",
    "    # Define the epsilon values\n",
    "    epsilons = [0.001, 4.5, 9, 2.8, 2.4, 0.001] #the first entry in this is for HC1, with f1_max.  The other entriest are for HC2-6 with F2 max\n",
    "\n",
    "solutions = []\n",
    "res_x = []\n",
    "for i in range(len(epsilons)): \n",
    "    print(i)\n",
    "    # set maximum values on parameters\n",
    "    if i == 0:\n",
    "        f1_max = maxes[i]\n",
    "    else:\n",
    "        f2_max = maxes[i]\n",
    "\n",
    "    #set epsilon values\n",
    "    epsilon = epsilons[i]\n",
    "\n",
    "    #solve the optimization problem, which differs for HC1 than the rest of the problems\n",
    "    if i ==0:\n",
    "        res = differential_evolution(opt_f1, bounds, args=(), tol=1e-2, mutation=(0.5, 1), recombination=0.7, strategy='best1bin', popsize=15, maxiter=1000, seed=2410, callback=None, disp=False, polish=True, init='latinhypercube', atol=0)\n",
    "    else:\n",
    "        res = differential_evolution(opt_f2, bounds, args=(), tol=1e-2, mutation=(0.5, 1), recombination=0.7, strategy='best1bin', popsize=15, maxiter=1000, seed=2410, callback=None, disp=False, polish=True, init='latinhypercube', atol=0)\n",
    "\n",
    "    res_x.append(res.x) #(C, H, N, O, FC, VM, Ash, T, RT, HT)\n",
    "    solutions.append((f1(res.x), f2(res.x)))    \n",
    "\n",
    "for j in range(len(solutions)):\n",
    "    print(solutions[j]) #(C, H, N, O, FC, VM, Ash, T, RT, HT)\n",
    "\n",
    "for j in range(len(solutions)): \n",
    "    print(res_x[j]) #(C, H, N, O, FC, VM, Ash, T, RT, HT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single objective optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code is used to test the objective functions, and does not generate results for the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [(53.9, 53.9), (8.9, 8.9), (1.7, 1.7), (33.7, 33.7), (13.8, 13.8), (84.8, 84.8), (1.4, 1.4), (200, 500), (10, 120), (2.5, 20)]\n",
    "res_x = []\n",
    "solutions = []\n",
    "print(bounds)\n",
    "\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        rei_obj = True\n",
    "    else:\n",
    "        rei_obj = False\n",
    "\n",
    "    # Define the optimization problem\n",
    "    def opt_single_obj(x):\n",
    "        if not rei_obj:\n",
    "            return 0-f1(x) #CSI\n",
    "        else:\n",
    "            return 0-f2(x) #REI\n",
    "        \n",
    "    res = differential_evolution(opt_single_obj, bounds, args=(), tol=1e-2, mutation=(0.5, 1), recombination=0.7, strategy='best1bin', popsize=15, maxiter=1000, seed=2410, callback=None, disp=False, polish=True, init='latinhypercube', atol=0)\n",
    "\n",
    "    res_x.append(res.x) #(C, H, N, O, FC, VM, Ash, T, RT, HT)\n",
    "    solutions.append((f1(res.x), f2(res.x)))    \n",
    "\n",
    "for j in range(len(solutions)):\n",
    "    print(solutions[j]) #(C, H, N, O, FC, VM, Ash, T, RT, HT)\n",
    "\n",
    "for j in range(len(solutions)): \n",
    "    print(res_x[j]) #(C, H, N, O, FC, VM, Ash, T, RT, HT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lognormal_samples(mean, sd, size):\n",
    "    # Calculate the parameters of the underlying normal distribution\n",
    "    mu = np.log(mean**2 / np.sqrt(sd**2 + mean**2))\n",
    "    sigma = np.sqrt(np.log((sd**2 / mean**2) + 1))\n",
    "\n",
    "    # Generate samples from the lognormal distribution\n",
    "    samples = np.random.lognormal(mu, sigma, size)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use data for HC1, HC6, BC1, BC6 from table 4\n",
    "def get_inputs():\n",
    "    if not c:\n",
    "        print(\"REI optimal\")\n",
    "        if p:\n",
    "            print(\"pyrolysis - BC1\")\n",
    "            C_dist = generate_lognormal_samples(53.9, 2.3, size=100000)\n",
    "            H_dist = generate_lognormal_samples(8.9, 0.4, size=100000)\n",
    "            N_dist = generate_lognormal_samples(1.7, 0.1, size=100000)\n",
    "            O_dist = generate_lognormal_samples(33.7, 3.0, size=100000)\n",
    "            FC_dist = generate_lognormal_samples(13.8, 0.7, size=100000)\n",
    "            VM_dist = generate_lognormal_samples(84.8, 0.8, size=100000)\n",
    "            Ash_dist = generate_lognormal_samples(1.4, 0, size=100000)\n",
    "            T_dist = generate_lognormal_samples(300.22, 0, size=100000)\n",
    "            RT_dist = generate_lognormal_samples(10.03, 0, size=100000)\n",
    "            HT_dist = generate_lognormal_samples(19.98, 0, size=100000)\n",
    "            inputs = np.column_stack((C_dist, H_dist, N_dist, O_dist, FC_dist, VM_dist, Ash_dist, T_dist, RT_dist, HT_dist))\n",
    "        else:\n",
    "            print(\"htc - HC1\")\n",
    "            C_dist = generate_lognormal_samples(53.9, 2.3, size=100000)\n",
    "            H_dist = generate_lognormal_samples(8.9, 0.4, size=100000)\n",
    "            N_dist = generate_lognormal_samples(1.7, 0.1, size=100000)\n",
    "            O_dist = generate_lognormal_samples(33.7, 3.0, size=100000)\n",
    "            WC_dist = generate_lognormal_samples(50.40, 0, size=100000)\n",
    "            VM_dist = generate_lognormal_samples(84.8, 0.8, size=100000)\n",
    "            FC_dist = generate_lognormal_samples(13.8, 0.7, size=100000)\n",
    "            Ash_dist = generate_lognormal_samples(1.4, 0, size=100000)\n",
    "            T_dist = generate_lognormal_samples(180.0, 0, size=100000)\n",
    "            RT_dist = generate_lognormal_samples(30.0, 0, size=100000)\n",
    "            inputs = np.column_stack((C_dist, H_dist, N_dist, O_dist, WC_dist, VM_dist, FC_dist, Ash_dist, T_dist, RT_dist))\n",
    "    else:\n",
    "        print(\"CSI optimal\")\n",
    "        if p:\n",
    "            print(\"pyrolysis - BC6\")\n",
    "            C_dist = generate_lognormal_samples(53.9, 2.3, size=100000)\n",
    "            H_dist = generate_lognormal_samples(8.9, 0.4, size=100000)\n",
    "            N_dist = generate_lognormal_samples(1.7, 0.1, size=100000)\n",
    "            O_dist = generate_lognormal_samples(33.7, 3.0, size=100000)\n",
    "            FC_dist = generate_lognormal_samples(13.8, 0.7, size=100000)\n",
    "            VM_dist = generate_lognormal_samples(84.8, 0.8, size=100000)\n",
    "            Ash_dist = generate_lognormal_samples(1.4, 0, size=100000)\n",
    "            T_dist = generate_lognormal_samples(319.33, 0, size=100000)\n",
    "            RT_dist = generate_lognormal_samples(90.39, 0, size=100000)\n",
    "            HT_dist = generate_lognormal_samples(17.15, 0, size=100000)\n",
    "            inputs = np.column_stack((C_dist, H_dist, N_dist, O_dist, FC_dist, VM_dist, Ash_dist, T_dist, RT_dist, HT_dist))\n",
    "        else:\n",
    "            print(\"htc - HC 6\")\n",
    "            C_dist = generate_lognormal_samples(53.9, 2.3, size=100000)\n",
    "            H_dist = generate_lognormal_samples(8.9, 0.4, size=100000)\n",
    "            N_dist = generate_lognormal_samples(1.7, 0.1, size=100000)\n",
    "            O_dist = generate_lognormal_samples(33.7, 3.0, size=100000)\n",
    "            WC_dist = generate_lognormal_samples(79.19, 0, size=100000)\n",
    "            VM_dist = generate_lognormal_samples(84.8, 0.8, size=100000)\n",
    "            FC_dist = generate_lognormal_samples(13.8, 0.7, size=100000)\n",
    "            Ash_dist = generate_lognormal_samples(1.4, 0, size=100000)\n",
    "            T_dist = generate_lognormal_samples(182.89, 0, size=100000)\n",
    "            RT_dist = generate_lognormal_samples(210.07, 0, size=100000)\n",
    "            inputs = np.column_stack((C_dist, H_dist, N_dist, O_dist, WC_dist, VM_dist, FC_dist, Ash_dist, T_dist, RT_dist))\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objectives and constraints\n",
    "def CSI(x):\n",
    "    outputs = my_model(inputs)\n",
    "    if p:\n",
    "        return outputs[:, 7] - (outputs[:, 2]/ outputs[:, 1]*12 + outputs[:, 3]/ outputs[:, 1]*12/16 + outputs[:, 4]/ outputs[:, 1]*12/14) / 3 * outputs[:, 0]\n",
    "    else:\n",
    "        return outputs[:, 7] - (outputs[:, 2]/ outputs[:, 1]*12 + outputs[:, 3]/ outputs[:, 1]*12/14 + outputs[:, 4]/ outputs[:, 1]*12/16) / 3 * outputs[:, 0]\n",
    "\n",
    "def REI(x):\n",
    "    outputs = my_model(inputs)\n",
    "    if p:\n",
    "        return outputs[:,5] * 1000 / (45 * (inputs[:,8] + (inputs[:,7] - 25)/inputs[:,9] ) + 113 + (inputs[:,7] - 25) * (4.186 * 0.05 + 0.00267 * inputs[:,8] + 1.22557) )\n",
    "    else:\n",
    "        return (outputs[:,5]+10) * 1000 / (45 * inputs[:,9] + (inputs[:,8] - 25) * (4.186 * inputs[:,4] / (100 - inputs[:,4]) + 0.00267 * inputs[:,8] + 1.22557) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100,000 samples for the monte carlo simulation are generated and reported in the res variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSI optimal\n",
      "pyrolysis - BC6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59.853630</td>\n",
       "      <td>4.818866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.748108</td>\n",
       "      <td>4.827039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59.974712</td>\n",
       "      <td>4.906418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57.235912</td>\n",
       "      <td>4.842136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.708244</td>\n",
       "      <td>4.804473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>57.962769</td>\n",
       "      <td>4.874658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>59.964016</td>\n",
       "      <td>4.762810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>55.863617</td>\n",
       "      <td>4.725340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>57.928028</td>\n",
       "      <td>4.734663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>56.816795</td>\n",
       "      <td>4.891623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         0\n",
       "0      59.853630  4.818866\n",
       "1      54.748108  4.827039\n",
       "2      59.974712  4.906418\n",
       "3      57.235912  4.842136\n",
       "4      59.708244  4.804473\n",
       "...          ...       ...\n",
       "99995  57.962769  4.874658\n",
       "99996  59.964016  4.762810\n",
       "99997  55.863617  4.725340\n",
       "99998  57.928028  4.734663\n",
       "99999  56.816795  4.891623\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = get_inputs()\n",
    "\n",
    "# Generate random input values\n",
    "CSI_value = pd.DataFrame(CSI(inputs))\n",
    "REI_value = pd.DataFrame(REI(inputs))\n",
    "\n",
    "# combine the two dataframes vertically\n",
    "res = pd.concat([CSI_value, REI_value], axis=1)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
